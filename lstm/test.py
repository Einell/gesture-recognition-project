import cv2
import numpy as np
import mediapipe as mp
from tensorflow.keras.models import load_model
import time
import threading  # ğŸ’¡ æ–°å¢ï¼šç”¨äºå¤šçº¿ç¨‹æ‰§è¡Œ
import gesture_control

# ================= é…ç½® =================
MODEL_PATH = 'gesture_lstm_model.keras'
CLASSES_PATH = 'lstm_classes.npy'
SEQUENCE_LENGTH = 20
THRESHOLD = 0.85
ACTION_COOLDOWN = 1.0
SKIP_FRAMES = 1  # ğŸ’¡ æ–°å¢ï¼šæ¯éš”2å¸§æ£€æµ‹ä¸€æ¬¡ï¼Œé™ä½è´Ÿè½½

# ================= åˆå§‹åŒ– MediaPipe =================
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles


# ================= ä¿®å¤ç‰ˆï¼šç‰¹å¾æå– (å¿…é¡»ä¸è®­ç»ƒé‡‡é›†ä¸€è‡´) =================
def extract_keypoints(results):
    """
    ä¸ get_lstm_features_3s.py é€»è¾‘ä¿æŒä¸€è‡´ï¼š
    1. ä¸­å¿ƒåŒ– (å‡å»æ‰‹è…•åæ ‡)
    2. å½’ä¸€åŒ– (é™¤ä»¥æœ€å¤§è·ç¦»)
    3. å·¦å³æ‰‹æ’åº
    """
    feature_vector = np.zeros(126)  # 2 * 21 * 3

    if not results.multi_hand_landmarks:
        return feature_vector

    for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
        # è·å–å·¦å³æ‰‹æ ‡ç­¾
        handedness = results.multi_handedness[idx].classification[0].label

        # 1. æå–åæ ‡
        lm_array = np.array([[lm.x, lm.y, lm.z] for lm in hand_landmarks.landmark])

        # 2. ä¸­å¿ƒåŒ–: ä»¥æ‰‹è…•(0)ä¸ºåŸç‚¹
        wrist = lm_array[0]
        lm_array = lm_array - wrist

        # 3. å½’ä¸€åŒ–: ä½¿ç”¨æœ€å¤§è·ç¦»è¿›è¡Œç¼©æ”¾ (ä¸ä½ çš„é‡‡é›†è„šæœ¬åŒ¹é…)
        max_dist = np.max(np.linalg.norm(lm_array, axis=1))
        if max_dist > 0:
            lm_array /= max_dist

        flat_features = lm_array.flatten()

        # 4. æ ¹æ®å·¦å³æ‰‹å¡«å…¥å¯¹åº”ä½ç½®
        if handedness == 'Left':
            feature_vector[0:63] = flat_features
        else:
            feature_vector[63:126] = flat_features

    return feature_vector


# ================= åŠ¨ä½œæ‰§è¡Œçº¿ç¨‹ =================
def run_action_in_thread(gesture, cap_ref, img_ref, landmarks_ref):
    """åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­è¿è¡Œï¼Œé˜²æ­¢ gesture_control é‡Œçš„ time.sleep å¡æ­»è§†é¢‘"""
    try:
        gesture_control.execute_gesture_action(gesture, cap_ref, img_ref, landmarks_ref)
    except Exception as e:
        print(f"Action Error: {e}")


# ================= ä¸»ç¨‹åº =================
def main():
    # 1. åŠ è½½æ¨¡å‹
    try:
        model = load_model(MODEL_PATH)
        # è®°å¾—åŠ  allow_pickle=True
        classes = np.load(CLASSES_PATH, allow_pickle=True)
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {classes}")
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    cap = cv2.VideoCapture(0)

    sequence = []
    last_action_time = 0
    current_action = "Waiting..."
    confidence_score = 0.0
    frame_count = 0  # ç”¨äºè·³å¸§è®¡æ•°

    with mp_hands.Hands(
            model_complexity=0,  # 0=Lite (æœ€å¿«), 1=Full
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
            max_num_hands=2
    ) as hands:

        print("ğŸ¥ å¯åŠ¨æˆåŠŸï¼æŒ‰ 'q' é€€å‡ºç¨‹åºã€‚")

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret: break

            # é•œåƒç¿»è½¬
            frame = cv2.flip(frame, 1)
            frame_count += 1

            # å›¾åƒé¢„å¤„ç†
            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            # ğŸ’¡ ä¼˜åŒ–ï¼šè·³å¸§æ£€æµ‹
            # åªæœ‰å½“å¸§æ•°èƒ½è¢« (SKIP_FRAMES + 1) æ•´é™¤æ—¶æ‰è¿è¡Œ MediaPipe
            # å…¶ä»–æ—¶å€™åªæ˜¾ç¤ºç”»é¢ï¼Œä¸å¤„ç†ï¼Œæå¤§æå‡æµç•…åº¦
            if frame_count % (SKIP_FRAMES + 1) == 0:
                image.flags.writeable = False
                results = hands.process(image)
                image.flags.writeable = True

                # ç‰¹å¾æå–ä¸é¢„æµ‹
                keypoints = extract_keypoints(results)
                sequence.append(keypoints)
                sequence = sequence[-SEQUENCE_LENGTH:]

                if len(sequence) == SEQUENCE_LENGTH:
                    # åªæœ‰æ£€æµ‹åˆ°æ‰‹çš„æ—¶å€™æ‰è¿›è¡Œé¢„æµ‹ï¼Œå‡å°‘å…¨0æ•°æ®çš„å¹²æ‰°
                    if results.multi_hand_landmarks:
                        input_data = np.expand_dims(sequence, axis=0)
                        res = model.predict(input_data, verbose=0)[0]
                        best_idx = np.argmax(res)
                        confidence_score = res[best_idx]
                        predicted_gesture = classes[best_idx]

                        # æ‰§è¡Œé€»è¾‘
                        if confidence_score > THRESHOLD:
                            # ----------------------------------------------------
                            # ğŸ’¡ ä¼˜åŒ– 1ï¼šè¿‡æ»¤â€œèƒŒæ™¯â€å’Œâ€œå†·å´ä¸­â€åŠ¨ä½œ
                            # ----------------------------------------------------
                            # å‡è®¾ä½ å¢åŠ äº† 'background' ç±»åˆ«
                            if predicted_gesture == 'background' or predicted_gesture == 'static':
                                current_action = "Static/Background"
                                # å³ä½¿ç½®ä¿¡åº¦é«˜ï¼Œä¹Ÿä¸æ‰§è¡Œä»»ä½•æ“ä½œ
                                pass

                            # ä¼˜åŒ– 2ï¼šå¦‚æœè¯†åˆ«å‡ºæœ‰æ•ˆçš„åŠ¨ä½œ
                            elif (time.time() - last_action_time) > ACTION_COOLDOWN:
                                current_action = predicted_gesture
                                print(f"ğŸš€ æ‰§è¡Œ: {predicted_gesture} ({confidence_score:.2f})")

                                first_hand = results.multi_hand_landmarks[0] if results.multi_hand_landmarks else None

                                # å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ Thread å¯åŠ¨åŠ¨ä½œ
                                action_thread = threading.Thread(
                                    target=run_action_in_thread,
                                    args=(predicted_gesture, cap, frame, first_hand)
                                )
                                action_thread.start()

                                last_action_time = time.time()
                    else:
                        # æ²¡æ‰‹çš„æ—¶å€™
                        current_action = "No Hand"
                        confidence_score = 0.0

            # ç»˜åˆ¶ UI (æ¯ä¸€å¸§éƒ½ç”»)
            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

            # å¦‚æœæœ‰ä¹‹å‰çš„æ£€æµ‹ç»“æœï¼Œå¯ä»¥ç”»ä¸€ä¸‹ï¼ˆå¯é€‰ï¼Œè¿™é‡Œä¸ºäº†æµç•…åº¦åªç”»ç®€å•çš„ï¼‰
            if 'results' in locals() and results.multi_hand_landmarks:
                for hand_landmarks in results.multi_hand_landmarks:
                    mp_drawing.draw_landmarks(image, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            # ä¿¡æ¯æ¡
            cv2.rectangle(image, (0, 0), (640, 40), (0, 0, 0), -1)
            color = (0, 255, 0) if (time.time() - last_action_time) > ACTION_COOLDOWN else (0, 0, 255)
            cv2.putText(image, f"{current_action} ({confidence_score:.2f})", (10, 30),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)

            cv2.imshow('Gesture Control', image)

            # ğŸ’¡ é€€å‡ºé€»è¾‘ï¼šä½¿ç”¨ waitKey(1) æé«˜å“åº”é€Ÿåº¦
            if cv2.waitKey(1) & 0xFF == ord('q'):
                print("æ­£åœ¨é€€å‡º...")
                break

    cap.release()
    cv2.destroyAllWindows()


if __name__ == '__main__':
    main()